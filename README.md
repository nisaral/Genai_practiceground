# Genai_practiceground
## transformer architecture
Transformer models use self-attention mechanisms to weigh the
importance of different words in a sentence, consisting of an encoder to
process input text and a decoder to generate output text, with parallel
processing enabling efficient training.

The Transformer is an encoder-decoder model designed for sequence-to-sequence tasks. It processes input and output sequences using stacks of identical layers, each leveraging attention and feed-forward networks. Figure 1 in the paper illustrates this structure (reproduced with permission for scholarly use).

Encoder
Structure: 6 identical layers (N=6), each with two sub-layers:
Multi-Head Self-Attention: Computes relationships between all input tokens.
Feed-Forward Network (FFN): Applies a fully connected layer to each position independently.
Residual Connections: Each sub-layer adds its input to its output (x + Sublayer(x)), followed by layer normalization.
Output: Produces a continuous representation z of dimension d_model=512 for each input token.
Decoder
Structure: Also 6 layers, each with three sub-layers:
Masked Multi-Head Self-Attention: Attends to previous positions only, ensuring auto-regressive generation (predictions for position i depend only on positions <i).
Encoder-Decoder Attention: Queries come from the decoder, keys/values from the encoder output, allowing the decoder to focus on input tokens.
Feed-Forward Network: Same as in the encoder.
Residual Connections and Normalization: Applied as in the encoder.
Output: Generates the output sequence one token at a time, using previously generated tokens as input.
Key Components
Scaled Dot-Product Attention:
Inputs: Query (Q), Key (K), Value (V) matrices of dimensions d_k and d_v.
Formula:
Attention
(
ð‘„
,
ð¾
,
ð‘‰
)
=
softmax
(
ð‘„
ð¾
ð‘‡
ð‘‘
ð‘˜
)
ð‘‰
Attention(Q,K,V)=softmax( 
d 
k
â€‹
 
â€‹
 
QK 
T
 
â€‹
 )V
Scaling by 
ð‘‘
ð‘˜
d 
k
â€‹
 
â€‹
 : Prevents large dot products from pushing softmax into low-gradient regions.
Efficiently computed in parallel using matrix operations.
Multi-Head Attention:
Projects Q, K, V into h=8 subspaces (d_k=d_v=d_model/h=64), applies attention in parallel, then concatenates and projects back.
Allows the model to capture different aspects of relationships (e.g., syntactic, semantic).
Formula:
MultiHead
(
ð‘„
,
ð¾
,
ð‘‰
)
=
Concat
(
head
1
,
.
.
.
,
head
â„Ž
)
ð‘Š
ð‘‚
MultiHead(Q,K,V)=Concat(head 
1
â€‹
 ,...,head 
h
â€‹
 )W 
O
 
where 
head
ð‘–
=
Attention
(
ð‘„
ð‘Š
ð‘–
ð‘„
,
ð¾
ð‘Š
ð‘–
ð¾
,
ð‘‰
ð‘Š
ð‘–
ð‘‰
)
head 
i
â€‹
 =Attention(QW 
i
Q
â€‹
 ,KW 
i
K
â€‹
 ,VW 
i
V
â€‹
 ).
Position-wise Feed-Forward Networks:
Applied independently to each position:
FFN
(
ð‘¥
)
=
max
â¡
(
0
,
ð‘¥
ð‘Š
1
+
ð‘
1
)
ð‘Š
2
+
ð‘
2
FFN(x)=max(0,xW 
1
â€‹
 +b 
1
â€‹
 )W 
2
â€‹
 +b 
2
â€‹
 
d_model=512 (input/output), d_ff=2048 (inner layer).
Positional Encoding:
Since thereâ€™s no recurrence, the model needs to know token positions.
Uses fixed sine/cosine functions:
ð‘ƒ
ð¸
(
ð‘
ð‘œ
ð‘ 
,
2
ð‘–
)
=
sin
â¡
(
ð‘
ð‘œ
ð‘ 
1000
0
2
ð‘–
/
ð‘‘
model
)
,
ð‘ƒ
ð¸
(
ð‘
ð‘œ
ð‘ 
,
2
ð‘–
+
1
)
=
cos
â¡
(
ð‘
ð‘œ
ð‘ 
1000
0
2
ð‘–
/
ð‘‘
model
)
PE(pos,2i)=sin( 
10000 
2i/d 
model
â€‹
 
 
pos
â€‹
 ),PE(pos,2i+1)=cos( 
10000 
2i/d 
model
â€‹
 
 
pos
â€‹
 )
Added to input embeddings, allowing the model to learn relative positions.
Embeddings and Softmax:
Input/output tokens are embedded into d_model=512 vectors.
Shared weights between input/output embeddings and the final softmax layer, scaled by 
ð‘‘
model
d 
model
â€‹
 
â€‹
 .
Advantages Over RNNs/CNNs
Parallelization: Self-attention processes all tokens simultaneously, unlike RNNs (O(n) sequential steps).
Constant Path Length: Connects any two positions with O(1) operations, vs. O(n) for RNNs or O(log_k(n)) for CNNs (Table 1).
Scalability: Handles long sequences better, though complexity is O(n^2Â·d) per layer (later optimized in models like Performer).

![image](https://github.com/user-attachments/assets/57f20652-4d16-41ba-8886-086d3692513b)
